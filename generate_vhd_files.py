"""
* This file reads the .csv file generated by the TensorFlow containing
* all the weights and biases of a specific NN and from this files it
* creates all files to the VHDL NN: ram, neuron, layer and the package 
* for instantiation in a top level file. 
* Each file is created individually for a layer 'l' and a neuron 'x'.
*
* The weights file are generated by the "convert_weights" Python file.
"""

import numpy as np
from os.path import join

layers = 5
layers_neurons = [6,30,24,20,5]
inputs = [6,6,30,24,20]
weights_folder = './1d48d172/rna_parameters/converted/'

result_folder = './vhd_files/'

for layer in range(layers):
    biases = np.genfromtxt(join(weights_folder + 'dense_' + str(layer) + '_bias_bin.csv'), delimiter=',', dtype='S')
    weights = np.genfromtxt(join(weights_folder + 'dense_' + str(layer) + '_kernel_bin.csv'), delimiter=',', dtype='S')

    for neuron in range(layers_neurons[layer]):
        with open(join(result_folder + 'neuron_l' + str(layer) + '_n' + str(neuron) + '.vhd'), 'a') as handle:
            handle.write("library ieee;\n"
                         "use ieee.std_logic_1164.all;\n"
                         "use ieee.numeric_std.all;\n"
                         "\n"
                         "library ieee_proposed;\n"
                         "use ieee_proposed.fixed_pkg.all;\n"
                         "\n"
                         "library amc_library;\n"
                         "use amc_library.data_types_pkg.all;\n"
                         "\n"
                         "entity neuron_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\tgeneric(\n"
                         "\t\tg_bits        : natural := c_bits;\n"
                         "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                         "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                         "\tport(\n"
                         "\t\ti_clk           : in std_logic;\n"
                         "\t\ti_rst           : in std_logic;\n"
                         "\t\ti_enable        : in std_logic;\n"
                         "\t\ti_fxp_data      : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\to_mac_done       : out std_logic;\n"
                         "\t\to_done          : out std_logic;\n"                         
                         "\t\to_fxp_data      : out std_logic_vector(g_bits-1 downto 0));\n"
                         "end neuron_l" + str(layer) + "_n" + str(neuron) + ";\n"
                         "\n"
                         "architecture bhv of neuron_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\n"
                         "\t--Control FSM signals\n")
            if layer == 0:
                handle.write("\ttype t_sm is (s_idle, s_get_weight, s_wait_weight, s_mac, s_wait_mac, s_mac_result, s_bias, s_relu, s_wait_relu, s_clear);\n")
            elif layer == 4:
                handle.write("\ttype t_sm is (s_idle, s_get_weight, s_wait_weight, s_mac, s_wait_mac, s_mac_result, s_bias, s_wait_bias, s_clear);\n")
            else:
                handle.write("\ttype t_sm is (s_idle, s_get_weight, s_wait_weight, s_mac, s_wait_mac, s_mac_result, s_bias, s_lut_tanh, s_wait_lut_tanh, s_clear);\n")
            
            handle.write("\tsignal r_sm             : t_sm := s_idle;\n"
                         "\tsignal r_sinapse_count	: integer := 0;\n"
                         "\tsignal r_done			: std_logic := '0';\n"
                         "\tconstant c_bias			: std_logic_vector(g_bits-1 downto 0) := \"" + str(biases[neuron].decode('utf-8')) + "\";\n"
                         "\tconstant c_inputs		: natural := " + str(inputs[layer]) + ";\n"
                         "\n"
                         "\t--RAM Signals\n"
                         "\tsignal r_wr				: std_logic							 		:= '0';\n"
                         "\tsignal r_addr			: std_logic_vector(5 downto 0) 				:= \"000000\";\n"
                         "\tsignal r_data_in_ram	: std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                         "\tsignal r_data_out_ram	    : std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                         "\n"
                         "\t--MAC Signals\n"
                         "\tsignal r_mac_enable		: std_logic									:= '0';\n"
                         "\tsignal r_mac_done			: std_logic									:= '1';\n"
                         "\tsignal r_mac_out			: std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                         "\n"
                         "\t--Bias Signal\n"
                         "\tsignal r_bias			: std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                         "\n"
                         "\t--Activation Function Signals\n")
            
            if layer == 0: 
                handle.write("\tsignal r_relu_enable	: std_logic									:= '0';\n"
                             "\tsignal r_relu_in		: std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                             "\tsignal r_relu_result	: std_logic_vector(g_bits-1 downto 0)	:= (others => '0');\n"
                             "\n")
            elif layer == 4:
                handle.write("\t--Last layer neurons have no activation functions inside them.")
                handle.write("\n")
            else:
                handle.write("\tsignal r_tanh_in        : std_logic_vector(g_bits-1 downto 0) := (others => '0');\n"
                             "\tsignal r_tanh_result    : std_logic_vector(g_bits-1 downto 0) := (others => '0');\n"
                             "\n")

            handle.write("component ram_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\tgeneric(\n"
                         "\t\tg_width       : natural := c_bits;\n"
                         "\t\tg_depth       : natural := 50;\n"
                         "\t\tg_addr_bits   : natural := 6);\n"
                         "\tport(\n"
                         "\t\ti_clk         : in std_logic;\n"
                         "\t\ti_wr			: in std_logic;\n"
                         "\t\ti_addr		: in std_logic_vector(g_addr_bits-1 downto 0);\n"
                         "\t\ti_data		: in std_logic_vector(g_width-1 downto 0);\n"
                         "\t\to_data		: out std_logic_vector(g_width-1 downto 0));\n"
                         "end component;\n"
                         "\n"
                         "component mac is\n"
                         "\tgeneric(\n"
                         "\t\tg_bits        : natural := c_bits;\n"
                         "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                         "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                         "\tport(\n"
                         "\t\ti_clk         : in std_logic;\n"
                         "\t\ti_rst         : in std_logic;\n"
                         "\t\ti_enable      : in std_logic;\n"
                         "\t\ti_data        : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\ti_weight		: in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\to_done		: out std_logic;\n"
                         "\t\to_data		: out std_logic_vector(g_bits-1 downto 0));\n"
                         "end component;\n"
                         "\n")

            if layer == 0:
                handle.write("component lut_relu is\n"
                             "\tgeneric(\n"
                             "\t\tg_bits        : natural := c_bits;\n"
                             "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                             "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                             "\tport(\n"
                             "\t\ti_clk         : in std_logic;\n"
                             "\t\ti_enable      : in std_logic;\n"
                             "\t\ti_value		: in std_logic_vector(g_bits-1 downto 0);\n"
                             "\t\to_result 	: out std_logic_vector(g_bits-1 downto 0));\n"
                             "end component;\n"
                             "\n")
            elif layer == 4:
                handle.write("\n")
            else:
                handle.write("component lut_tanh is\n"
                             "\tgeneric(\n"
                             "\t\tg_bits        : natural := c_bits;\n"
                             "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                             "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                             "\tport(\n"
                             "\t\ti_address     : in std_logic_vector(g_bits-1 downto 0);\n"
                             "\t\to_output      : out std_logic_vector(g_bits-1 downto 0));\n"
                             "end component;\n"
                             "\n")
            handle.write("begin\n"
                         "\n"
                         "\tram_n" + str(neuron) + " : ram_l" + str(layer) + "_n" + str(neuron) + " port map(i_clk, r_wr, r_addr, r_data_in_ram, r_data_out_ram);\n"
                         "\tmac_n" + str(neuron) + " : mac port map(i_clk, i_rst, r_mac_enable, i_fxp_data, r_data_out_ram, r_mac_done, r_mac_out);\n")
            
            if layer == 0:
                handle.write("\tact_relu	: lut_relu port map(i_clk, r_relu_enable, r_relu_in, r_relu_result);\n"
                             "\n")
            elif layer == 4:
                handle.write("\n")
            else:
                handle.write("\tact_lut_tanh : lut_tanh port map(r_tanh_in, r_tanh_result);\n"
                             "\n")
            
            handle.write("\tp_neuron : process(i_clk, i_enable, r_mac_done)\n"
                         "\tbegin\n"
                         "\t\tif rising_edge(i_clk) then\n"
                         "\t\t\tcase r_sm is\n"
                         "\t\t\t\twhen s_idle =>\n"
                         "\t\t\t\t\tif i_enable = '1' then\n"
                         "\t\t\t\t\t\tr_sm <= s_get_weight;\n"
                         "\t\t\t\t\telse\n"
                         "\t\t\t\t\t\tr_sm <= s_idle;\n"
                         "\t\t\t\t\tend if;\n"
                         "\n"
                         "\t\t\t\twhen s_get_weight =>\n"
                         "\t\t\t\t\tr_addr 				<= std_logic_vector(to_unsigned(r_sinapse_count, r_addr'length));\n"
                         "\t\t\t\t\tr_sinapse_count 	<= r_sinapse_count + 1;\n"
                         "\t\t\t\t\tr_sm 					<= s_wait_weight;\n"
                         "\n"
                         "\t\t\t\twhen s_wait_weight =>\n"
                         "\t\t\t\t\tr_sm				<= s_mac;\n"
                         "\n"
                         "\t\t\t\twhen s_mac =>\n"
                         "\t\t\t\t\tr_mac_enable <= '1';\n"
                         "\t\t\t\t\tr_sm 		 <= s_wait_mac;\n"
                         "\n"
                         "\t\t\t\twhen s_wait_mac =>\n"
                         "\t\t\t\t\tr_mac_enable <= '0';\n"
                         "\t\t\t\t\tr_sm 		 <= s_mac_result;\n"
                         "\n"
                         "\t\t\t\twhen s_mac_result =>\n"
                         "\t\t\t\t\tif r_mac_done = '1' then\n"
                         "\t\t\t\t\t\tif r_sinapse_count < c_inputs then\n"
                         "\t\t\t\t\t\t\tr_sm <= s_get_weight;\n"
                         "\t\t\t\t\t\telse\n"
                         "\t\t\t\t\t\t\tr_sm <= s_bias;\n"
                         "\t\t\t\t\t\tend if;\n"
                         "\t\t\t\t\telse\n"
                         "\t\t\t\t\t\tr_sm <= s_mac;\n"
                         "\t\t\t\t\tend if;\n"
                         "\n"
                         "\t\t\t\twhen s_bias =>\n"
                         "\t\t\t\t\tr_bias <= to_slv(resize(to_sfixed(r_mac_out, g_fxp_high, g_fxp_low) + to_sfixed(c_bias, g_fxp_high, g_fxp_low), g_fxp_high, g_fxp_low));\n")
            if layer == 0:
                handle.write("\t\t\t\t\tr_sm   <= s_relu;\n"
                             "\n"
                             "\t\t\t\twhen s_relu =>\n"
                             "\t\t\t\t\tr_mac_enable	<= '0';\n"
                             "\t\t\t\t\tr_relu_enable 	<= '1';\n"
                             "\t\t\t\t\tr_relu_in		<= r_bias;\n"
                             "\t\t\t\t\tr_sm 			<= s_wait_relu;\n"
                             "\n"
                             "\t\t\t\twhen s_wait_relu =>\n"
                             "\t\t\t\t\tr_relu_enable 	<= '0';\n"
                             "\t\t\t\t\tr_done			<= '1';\n"
                             "\t\t\t\t\tr_sm 			<= s_clear;\n"
                             "\n")
            elif layer == 4:
                handle.write("\t\t\t\t\tr_sm <= s_wait_bias;\n"
                             "\n"
                             "\t\t\t\twhen s_wait_bias => \n"
                             "\t\t\t\t\tr_done <= '1';\n"
                             "\t\t\t\t\tr_sm <= s_clear;\n"
                             "\n")

            else:
                handle.write("\t\t\t\t\tr_sm   <= s_lut_tanh;\n"
                             "\n"
                             "\t\t\t\twhen s_lut_tanh =>\n"
                             "\t\t\t\t\tr_tanh_in   <= r_bias;\n"
                             "\t\t\t\t\tr_sm        <= s_wait_lut_tanh;\n"
                             "\n"
                             "\t\t\t\twhen s_wait_lut_tanh => \n"
                             "\t\t\t\t\tr_done  <= '1';\n"
                             "\t\t\t\t\tr_sm    <= s_clear;\n"
                             "\n")
            handle.write("\t\t\t\twhen s_clear =>\n"
                         "\t\t\t\t\tr_done 			<= '0';\n"
                         "\t\t\t\t\tr_sinapse_count <= 0;\n"
                         "\t\t\t\t\tr_mac_enable 	<= '0';\n"
                         "\t\t\t\t\tr_sm 			<= s_idle;\n"
                         "\n"
                         "\t\t\t\twhen others =>\n"
                         "\t\t\t\t\tr_sm <= s_idle;\n"
                         "\n"
                         "\t\t\tend case;\n"
                         "\t\tend if;\n"
                         "\tend process p_neuron;\n"
                         "\n"
                         "\to_mac_done	<= r_mac_done;\n"
                         "\to_done		<= r_done;\n")
            if layer == 0:
                handle.write("\to_fxp_data 	<= r_relu_result;\n"
                             "\n"
                             "end bhv;")
            elif layer == 4:
                handle.write("\to_fxp_data 	<= r_bias;\n"
                             "\n"
                             "end bhv;")
            else:
                handle.write("\to_fxp_data <= r_tanh_result;"
                             "\n"
                             "end bhv;")

        with open(join(result_folder + 'ram_l' + str(layer) + '_n' + str(neuron) + '.vhd'), 'a') as handle:
            temp = []
            for line in range(inputs[layer]):
                temp.append(weights[line][neuron].decode('utf-8'))
            
            handle.write("library ieee;\n"
                         "use ieee.std_logic_1164.all;\n"
                         "use ieee.numeric_std.all;\n"
                         "\n"
                         "library amc_library;\n"
                         "use amc_library.data_types_pkg.all;\n"
                         "\n"
                         "entity ram_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\tgeneric(\n"
                         "\t\tg_width   : natural := c_bits;\n"
                         "\t\tg_depth 	: natural := 50;\n"
                         "\t\tg_addr_bits : natural := 5);\n"
                         "\tport(\n"
                         "\t\ti_clk			: in std_logic;\n"
                         "\t\ti_wr			: in std_logic;\n"
                         "\t\ti_addr		: in std_logic_vector(g_addr_bits-1 downto 0);\n"
                         "\t\ti_data		: in std_logic_vector(g_width-1 downto 0);\n"
                         "\t\to_data		: out std_logic_vector(g_width-1 downto 0));\n"
                         "end ram_l" + str(layer) + "_n" + str(neuron) + ";\n"
                         "\n"
                         "architecture rtl of ram_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\ttype t_mem is array (0 to g_depth-1) of std_logic_vector(g_width-1 downto 0);\n"
                         "\tsignal r_mem : t_mem := (" + str(temp).replace('\'', '\"').replace('[', '').replace(']','') + " , others => (others => '0'));\n"
                         "\n"
                         "begin\n"
                         "\tp_ram : process(i_clk)\n"
                         "\tbegin\n"
                         "\t\tif rising_edge(i_clk) then\n"
                         "\t\t\tif (i_wr='1') then\n"
                         "\t\t\t\tr_mem(to_integer(unsigned(i_addr))) <= i_data;\n"
                         "\t\t\tend if;\n"
                         "\t\t\to_data <= r_mem(to_integer(unsigned(i_addr)));\n"
                         "\t\tend if;\n"
                         "\tend process p_ram;\n"
                         "end rtl;\n")

        with open(join(result_folder + 'package.txt'), 'a') as handle:
            handle.write("component ram_l" + str(layer) + "_n" + str(neuron) + " is\n"
                         "\tgeneric(\n"
                         "\t\tg_width       : natural := c_bits;\n"
                         "\t\tg_depth       : natural := 50;\n"
                         "\t\tg_addr_bits   : natural := 6);\n"
                         "\tport(\n"
                         "\t\ti_clk         : in std_logic;\n"
                         "\t\ti_wr			: in std_logic;\n"
                         "\t\ti_addr		: in std_logic_vector(g_addr_bits-1 downto 0);\n"
                         "\t\ti_data		: in std_logic_vector(g_width-1 downto 0);\n"
                         "\t\to_data		: out std_logic_vector(g_width-1 downto 0));\n"
                         "end component;\n"
                         "\n")

            if layer == 0:
                handle.write("component lut_relu is\n"
                             "\tgeneric(\n"
                             "\t\tg_bits        : natural := c_bits;\n"
                             "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                             "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                             "\tport(\n"
                             "\t\ti_clk         : in std_logic;\n"
                             "\t\ti_enable      : in std_logic;\n"
                             "\t\ti_value		: in std_logic_vector(g_bits-1 downto 0);\n"
                             "\t\to_result 	: out std_logic_vector(g_bits-1 downto 0));\n"
                             "end component;\n"
                             "\n")
            elif layer == 4:
                handle.write("\n")
            else:
                handle.write("component lut_tanh is\n"
                             "\tgeneric(\n"
                             "\t\tg_bits        : natural := c_bits;\n"
                             "\t\tg_fxp_high    : natural := c_fxp_high;\n"
                             "\t\tg_fxp_low     : integer := c_fxp_low);\n"
                             "\tport(\n"
                             "\t\ti_address     : in std_logic_vector(g_bits-1 downto 0);\n"
                             "\t\to_output      : out std_logic_vector(g_bits-1 downto 0));\n"
                             "end component;\n"
                             "\n")

    with open(join(result_folder + 'layer_l' + str(layer) + '.vhd'), 'a') as handle:
        handle.write("library ieee;\n"
                         "use ieee.std_logic_1164.all;\n"
                         "use ieee.numeric_std.all;\n"
                         "\n"
                         "library ieee_proposed;\n"
                         "use ieee_proposed.fixed_pkg.all;\n"
                         "\n"
                         "library amc_library;\n"
                         "use amc_library.data_types_pkg.all;\n"
                         "\n"
                         "entity layer_l" + str(layer) + " is\n"
                         "\tgeneric(\n"
                         "\t\tg_bits           : natural := c_bits;\n"
                         "\t\tg_fxp_high 	   : natural := c_fxp_high;\n"
                         "\t\tg_fxp_low        : integer := c_fxp_low);\n"
                         "\tport(\n"
                         "\t\ti_clk			: in std_logic;\n"
                         "\t\ti_rst         : in std_logic;\n"
                         "\t\ti_enable      : in std_logic;\n"
                         "\t\ti_fxp         : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\to_mac_done    : out std_logic;\n"
                         "\t\to_done        : out std_logic;\n")
        if layer == 4:
            handle.write("\t\to_result			: out std_logic_vector(6 downto 0));\n")
        else:
            for out in range(layers_neurons[layer]-1):
                handle.write("\t\to_fxp_n" + str(out) + " : out std_logic_vector(g_bits-1 downto 0);\n")
            handle.write("\t\to_fxp_n" + str(range(layers_neurons[layer])[-1]) + " : out std_logic_vector(g_bits-1 downto 0));\n")
        
        handle.write("end layer_l" + str(layer) + ";\n"
                     "\n"
                     "architecture bhv of layer_l" + str(layer) + " is\n"
                     "\n")
        if layer == 4:
            handle.write("\ttype t_sm is (s_idle, s_get_data, s_sinapse, s_wait_sinapse, s_max, s_wait_max, s_clear);\n"
                         "\tsignal r_sm : t_sm := s_idle;\n"
                         "\n")
        else:
            handle.write("\ttype t_sm is (s_idle, s_get_data, s_sinapse, s_wait_sinapse, s_wait_activation, s_clear);\n"
                     "\tsignal r_sm : t_sm := s_idle;\n"
                     "\n")
        
        for out in range(layers_neurons[layer]):
            handle.write("\tsignal r_enable_n" + str(out) + " : std_logic := '0';\n")
        handle.write("\n")
        for out in range(layers_neurons[layer]):
            handle.write("\tsignal r_mac_n" + str(out) + " : std_logic := '0';\n")
        handle.write("\tsignal r_mac_done : std_logic := '0';\n"
                     "\n")
        for out in range(layers_neurons[layer]):
            handle.write("\tsignal r_done_n" + str(out) + " : std_logic := '0';\n")
        handle.write("\tsignal r_done : std_logic := '0';\n")
        handle.write("\n")
        for out in range(layers_neurons[layer]):
            handle.write("\tsignal r_result_n" + str(out) + " : std_logic_vector(g_bits-1 downto 0) := (others => '0');\n")
        handle.write("\n")
        handle.write("\tsignal r_sinapse : integer range 0 to " + str(inputs[layer]) + " := 0;\n"
                     "\tconstant c_inputs         : natural := " + str(inputs[layer]) + ";\n"
                     "\n")

        if layer == 4:
            handle.write("\tsignal r_max_enable : std_logic := '0';\n"
                         "\tsignal r_max_result	: std_logic_vector(6 downto 0);\n"
                         "\n")

        if layer == 4:
            handle.write("\tcomponent max is\n"
                         "\t\tgeneric(\n"
                         "\t\t\tg_bits        : natural := c_bits;\n"
                         "\t\t\tg_fxp_high    : natural := c_fxp_high;\n"
                         "\t\t\tg_fxp_low     : integer := c_fxp_low);\n"
                         "\t\tport(\n"
                         "\t\t\ti_enable    : in std_logic;\n"
                         "\t\t\ti_neuron_1  : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\ti_neuron_2  : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\ti_neuron_3  : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\ti_neuron_4  : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\ti_neuron_5  : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\to_result    : out std_logic_vector(6 downto 0));\n"
                         "\tend component;\n"
                         "\n")

        for out in range(layers_neurons[layer]):
            handle.write("\tcomponent neuron_l" + str(layer) + "_n" + str(out) + " is\n"
                         "\t\tgeneric(\n"
                         "\t\t\tg_bits        : natural := c_bits;\n"
                         "\t\t\tg_fxp_high    : natural := c_fxp_high;\n"
                         "\t\t\tg_fxp_low     : integer := c_fxp_low);\n"
                         "\t\tport(\n"
                         "\t\t\ti_clk         : in std_logic;\n"
                         "\t\t\ti_rst         : in std_logic;\n"
                         "\t\t\ti_enable      : in std_logic;\n"
                         "\t\t\to_mac_done    : out std_logic;\n"
                         "\t\t\to_done        : out std_logic;\n"
                         "\t\t\ti_fxp_data    : in std_logic_vector(g_bits-1 downto 0);\n"
                         "\t\t\to_fxp_data    : out std_logic_vector(g_bits-1 downto 0));\n"
                         "\tend component;\n\n")
        handle.write("begin\n")
        if layer == 4:
            handle.write("\tact_max	: maX port map(r_max_enable, r_result_n0, r_result_n1, r_result_n2, r_result_n3, r_result_n4, r_max_result);\n")
        for out in range(layers_neurons[layer]):
            handle.write("\tn" + str(out) + " : neuron_l" + str(layer) + "_n" + str(out) + " port map(i_clk, i_rst, r_enable_n" + str(out) + ", r_mac_n" + str(out) + ", r_done_n" + str(out) + ", i_fxp, r_result_n" + str(out) + ");\n")
        handle.write("\n"
                    "\tp_layer : process(i_clk, i_enable)\n"
                     "\tbegin\n"
                     "\t\tif rising_edge(i_clk) then\n"
                     "\t\t\tcase r_sm is\n"
                     "\t\t\t\twhen s_idle =>\n"
                     "\t\t\t\t\tif i_enable = '1' then\n"
                     "\t\t\t\t\t\tr_sm <= s_get_data;\n"
                     "\t\t\t\t\telse\n"
                     "\t\t\t\t\t\tr_sm <= s_idle;\n"
                     "\t\t\t\t\tend if;\n"
                     "\n"
                     "\t\t\t\twhen s_get_data =>\n"
                     "\t\t\t\t\tr_mac_done  <= '0';\n"
                     "\t\t\t\t\tr_sm        <= s_sinapse;\n"
                     "\n"
                     "\t\t\t\twhen s_sinapse =>\n")
        for out in range(layers_neurons[layer]):
            handle.write("\t\t\t\t\tr_enable_n" + str(out) + " <= '1';\n")
        handle.write("\t\t\t\t\tr_sinapse   <= r_sinapse + 1;\n"
                     "\t\t\t\t\tr_sm        <= s_wait_sinapse;\n"
                     "\n"
                     "\t\t\t\twhen s_wait_sinapse =>\n"
                     "\t\t\t\t\tif")
        for out in range(layers_neurons[layer]-1):
            handle.write(" (r_mac_n" + str(out) + " = '1') and")
        handle.write(" (r_mac_n" + str(range(layers_neurons[layer])[-1]) + " = '1')")        
        handle.write(" then\n"
                     "\t\t\t\t\t\tr_mac_done <= '1';\n"
                     "\t\t\t\t\t\tif r_sinapse < c_inputs then\n"
                     "\t\t\t\t\t\t\tr_sm <= s_get_data;\n"
                     "\t\t\t\t\t\telse\n")

        if layer == 4:
            handle.write("\t\t\t\t\t\t\tr_sm <= s_max;\n"
                         "\t\t\t\t\t\tend if;\n"
                         "\t\t\t\t\telse\n"
                         "\t\t\t\t\t\tr_sm <= s_wait_sinapse;\n"
                         "\t\t\t\tend if;\n"
                         "\n")
        else:
            handle.write("\t\t\t\t\t\t\tr_sm <= s_wait_activation;\n"
                         "\t\t\t\t\t\tend if;\n"
                         "\t\t\t\t\telse\n"
                         "\t\t\t\t\t\tr_sm <= s_wait_sinapse;\n"
                         "\t\t\t\t\tend if;\n"
                         "\n")
        
        if layer == 4:
            handle.write("\t\t\t\twhen s_max =>\n"
                        "\t\t\t\t\tr_mac_done <= '0';\n"
                        "\t\t\t\t\tif")
            for out in range(layers_neurons[layer]-1):
                handle.write(" (r_done_n" + str(out) + " = '1') and")
            handle.write(" (r_done_n" + str(range(layers_neurons[layer])[-1]) + " = '1')")
            handle.write(" then \n"
                         "\t\t\t\t\t\tr_max_enable <= '1';\n"
                         "\t\t\t\t\t\tr_sm <= s_wait_max;\n"
                         "\t\t\t\t\telse\n"
                         "\t\t\t\t\t\tr_sm <= s_max;\n"
                         "\t\t\t\t\t\tr_max_enable <= '0';\n"
                         "\t\t\t\t\tend if;\n"
                         "\n"
                         "\t\t\t\twhen s_wait_max =>\n"
                         "\t\t\t\t\tr_max_enable <= '0';\n"
                         "\t\t\t\t\tr_done <= '1';\n"
                         "\t\t\t\t\tr_sm <= s_clear;\n"
                         "\n")
        else:
            handle.write("\t\t\t\twhen s_wait_activation =>\n"
                     "\t\t\t\t\tr_mac_done <= '0';\n"
                     "\t\t\t\t\tif")
            for out in range(layers_neurons[layer]-1):
                handle.write(" (r_done_n" + str(out) + " = '1') and")
            handle.write(" (r_done_n" + str(range(layers_neurons[layer])[-1]) + " = '1')")
            handle.write(" then\n"
                        "\t\t\t\t\t\tr_done <= '1';\n"
                        "\t\t\t\t\t\tr_sm   <= s_clear;\n"
                        "\t\t\t\t\telse\n"
                        "\t\t\t\t\t\tr_sm <= s_wait_activation;\n"
                        "\t\t\t\t\t\tr_done <= '0';\n"
                        "\t\t\t\t\tend if;\n"
                        "\n")

        handle.write("\t\t\t\twhen s_clear =>\n"
                     "\t\t\t\t\tr_mac_done <= '0';\n")
        if layer == 4:
            handle.write("\t\t\t\t\tr_max_enable <= '0';\n") 

        for out in range(layers_neurons[layer]):
            handle.write("\t\t\t\t\tr_enable_n" + str(out) + " <= '0';\n")
        handle.write("\t\t\t\t\tr_done <= '0';\n"
                     "\t\t\t\t\tr_sinapse <= 0;\n"
                     "\t\t\t\t\tr_sm      <= s_idle;\n"
                     "\n"
                     "\t\t\t\twhen others =>\n"
                     "\t\t\t\t\t r_sm <= s_idle;\n"
                     "\t\t\tend case;\n"
                     "\t\tend if;\n"
                     "\tend process p_layer;\n"
                     "\n"
                     "\to_done      <= r_done;\n"
                     "\to_mac_done  <= r_mac_done;\n")
        
        if layer == 4:
            handle.write("\to_result <= r_max_result;\n"
                         "end bhv;")
        else:
            for out in range(layers_neurons[layer]):
                handle.write("\to_fxp_n" + str(out) + " <= r_result_n" + str(out) + ";\n")
            handle.write("\n"
                        "end bhv;")

        


print('Files successfully generated. Enjoy your circuit!') 